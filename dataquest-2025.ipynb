{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-23T11:58:51.242523Z",
     "iopub.status.busy": "2025-10-23T11:58:51.242006Z",
     "iopub.status.idle": "2025-10-23T11:59:17.912367Z",
     "shell.execute_reply": "2025-10-23T11:59:17.910752Z",
     "shell.execute_reply.started": "2025-10-23T11:58:51.242496Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/tf-idf-result/tfidf_word.joblib\n",
      "/kaggle/input/tf-idf-result/tfidf_word_tr.npz\n",
      "/kaggle/input/tf-idf-result/tfidf_char.joblib\n",
      "/kaggle/input/tf-idf-result/tfidf_char_te.npz\n",
      "/kaggle/input/tf-idf-result/tfidf_char_tr.npz\n",
      "/kaggle/input/tf-idf-result/tfidf_word_te.npz\n",
      "/kaggle/input/bert-embeddings/bert_embeddings.npz\n",
      "/kaggle/input/svd-result/tfidf_svd_word.npz\n",
      "/kaggle/input/svd-result/svd_char.joblib\n",
      "/kaggle/input/svd-result/svd_word.joblib\n",
      "/kaggle/input/svd-result/tfidf_svd_char.npz\n",
      "/kaggle/input/objective-quest-2025-dataset/sample_submission.csv\n",
      "/kaggle/input/objective-quest-2025-dataset/train.csv\n",
      "/kaggle/input/objective-quest-2025-dataset/test.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_37/1229163428.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/input'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/os.py\u001b[0m in \u001b[0;36m_walk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/os.py\u001b[0m in \u001b[0;36m_walk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/os.py\u001b[0m in \u001b[0;36m_walk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/os.py\u001b[0m in \u001b[0;36m_walk\u001b[0;34m(top, topdown, onerror, followlinks)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-23T11:59:17.912910Z",
     "iopub.status.idle": "2025-10-23T11:59:17.913222Z",
     "shell.execute_reply": "2025-10-23T11:59:17.913074Z",
     "shell.execute_reply.started": "2025-10-23T11:59:17.913057Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os \n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-23T11:59:17.914053Z",
     "iopub.status.idle": "2025-10-23T11:59:17.914277Z",
     "shell.execute_reply": "2025-10-23T11:59:17.914185Z",
     "shell.execute_reply.started": "2025-10-23T11:59:17.914175Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# list semua folder di /kaggle/input\n",
    "print(os.listdir(\"/kaggle/input\"))\n",
    "print(os.listdir(\"/kaggle/input/objective-quest-2025-dataset\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-23T11:59:17.915191Z",
     "iopub.status.idle": "2025-10-23T11:59:17.915483Z",
     "shell.execute_reply": "2025-10-23T11:59:17.915334Z",
     "shell.execute_reply.started": "2025-10-23T11:59:17.915318Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# load CSV\n",
    "BASE = Path(\"/kaggle/input/objective-quest-2025-dataset\")\n",
    "train = pd.read_csv(BASE/\"train.csv\")\n",
    "test  = pd.read_csv(BASE/\"test.csv\")\n",
    "sample_submission = pd.read_csv(BASE/\"sample_submission.csv\")\n",
    "\n",
    "print(train.head())\n",
    "print(test.head())\n",
    "print(sample_submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-23T11:59:17.916492Z",
     "iopub.status.idle": "2025-10-23T11:59:17.916713Z",
     "shell.execute_reply": "2025-10-23T11:59:17.916626Z",
     "shell.execute_reply.started": "2025-10-23T11:59:17.916617Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load texts\n",
    "texts_dir = BASE / \"file_putusan\"\n",
    "id2text = {}\n",
    "for p in texts_dir.rglob(\"*\"):\n",
    "    if p.is_file():\n",
    "        try:\n",
    "            id2text[p.stem] = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        except:\n",
    "            id2text[p.stem] = p.read_text(encoding=\"latin-1\", errors=\"ignore\")\n",
    "\n",
    "train[\"text\"] = train[\"id\"].map(id2text).fillna(\"\")\n",
    "test[\"text\"]  = test[\"id\"].map(id2text).fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-23T11:59:17.917651Z",
     "iopub.status.idle": "2025-10-23T11:59:17.917951Z",
     "shell.execute_reply": "2025-10-23T11:59:17.917776Z",
     "shell.execute_reply.started": "2025-10-23T11:59:17.917767Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleaning dokumen putusan (ringkas & aman untuk BERT/TF-IDF):\n",
    "    - Hapus header/footer umum (Halaman x dari y, Direktori MA, dll.)\n",
    "    - Hapus email & nomor telepon panjang\n",
    "    - Normalisasi '2 (dua)' -> '2'\n",
    "    - Rapikan spasi, newline, dan tanda hubung\n",
    "    - TIDAK melakukan lowercase (biar fleksibel: BERT cased vs TF-IDF)\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str) or not s:\n",
    "        return \"\"\n",
    "\n",
    "    # Normalisasi newline\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "    # Header/footer & boilerplate umum\n",
    "    # Hapus nomor halaman\n",
    "    s = re.sub(r\"Halaman\\s+\\d+\\s+dari\\s+\\d+.*?\\n\", \" \", s, flags=re.I)\n",
    "    # Hapus direktori putusan\n",
    "    s = re.sub(r\"Direktori Putusan Mahkamah Agung.*?\\n\", \" \", s, flags=re.I)\n",
    "    # Hapus header \"Mahkamah Agung Republik Indonesia\" meski kepotong/typo\n",
    "    s = re.sub(r\"mah?kamah.*agung.*indonesi[a-z]*\", \" \", s, flags=re.I)\n",
    "    # Hapus kata 'PUTUSAN' (sering dicetak besar/berjarak)\n",
    "    s = re.sub(r\"\\bP\\s*U\\s*T\\s*U\\s*S\\s*A\\s*N\\b\", \" \", s, flags=re.I)\n",
    "\n",
    "    # Hapus email & nomor telepon panjang\n",
    "    s = re.sub(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\", \" \", s)\n",
    "    s = re.sub(r\"\\b(?:\\+?62|0)\\d{8,}\\b\", \" \", s)\n",
    "\n",
    "    # Normalisasi angka “2 (dua)” -> “2” (jaga angka penting)\n",
    "    s = re.sub(r\"(\\d+)\\s*\\(\\s*[A-Za-z\\s]+\\s*\\)\", r\"\\1\", s)\n",
    "\n",
    "    # Rapikan tanda hubung & spasi\n",
    "    s = re.sub(r\"\\s*-\\s*\", \"-\", s)     # “laki - laki” -> “laki-laki”\n",
    "    s = re.sub(r\"[ \\t]+\", \" \", s)      # multi-spasi -> satu spasi\n",
    "    s = re.sub(r\"\\n{2,}\", \"\\n\", s)     # newline berturut -> 1 newline\n",
    "\n",
    "    return s.strip()\n",
    "\n",
    "train[\"text_bert\"] = train[\"text\"].map(clean)\n",
    "test[\"text_bert\"]  = test[\"text\"].map(clean)\n",
    "\n",
    "train[\"text_tfidf\"] = train[\"text_bert\"].str.lower()\n",
    "test[\"text_tfidf\"]  = test[\"text_bert\"].str.lower()\n",
    "\n",
    "# save\n",
    "\n",
    "train.to_pickle(\"train_clean.pkl\")\n",
    "test.to_pickle(\"test_clean.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-23T11:59:17.919344Z",
     "iopub.status.idle": "2025-10-23T11:59:17.919548Z",
     "shell.execute_reply": "2025-10-23T11:59:17.919461Z",
     "shell.execute_reply.started": "2025-10-23T11:59:17.919452Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "\n",
    "# Triggers & units \n",
    "RE_AMAR_TRIG   = re.compile(r\"(menjatuhkan\\s+(?:pidana(?:\\s+penjara)?|hukuman|kurungan)|menghukum)\", re.I)\n",
    "RE_TUNTUT_TRIG = re.compile(r\"(menuntut|tuntut\\w*)\", re.I)\n",
    "\n",
    "RE_YEARS  = re.compile(r\"(\\d+)\\s*(tahun|thn|th)\\b\", re.I)\n",
    "RE_MONTHS = re.compile(r\"(\\d+)\\s*(bulan|bln|bl)\\b\", re.I)\n",
    "RE_DAYS   = re.compile(r\"(\\d+)\\s*hari\\b\", re.I)\n",
    "\n",
    "# Pasal + UU (robust, ambil banyak)\n",
    "RE_PASAL_UU = re.compile(\n",
    "    r\"pasal\\s+(\\d+[A-Za-z]?)\"                              # pasal 127 / 127A\n",
    "    r\"(?:\\s+ayat\\s*\\(\\s*\\d+\\s*\\))?\"                        # opsional ayat\n",
    "    r\"(?:\\s+huruf\\s*[a-z])?\"                               # opsional huruf\n",
    "    r\".{0,120}?\"\n",
    "    r\"(?:uu|undang-?undang)\\s*(?:no\\.?|nomor)?\\s*(\\d+)\"    # UU nomor\n",
    "    r\"(?:\\s*(?:tahun)?\\s*(\\d{4}))?\",                       # opsional tahun\n",
    "    re.I | re.S\n",
    ")\n",
    "\n",
    "# Denda & subsider\n",
    "RE_RP_NUMBER = re.compile(r\"(?:rp\\.?|rupiah)\\s*([0-9\\.\\,]+)\", re.I)\n",
    "RE_SCALED_MONEY = re.compile(\n",
    "    r\"(?:(?:sebesar|senilai)\\s*)?([0-9\\.\\,]+)\\s*(ribu|juta|miliar|milyar)\\s*(?:rupiah|rp\\.?)?\",\n",
    "    re.I\n",
    ")\n",
    "SCALE = {\"ribu\":1_000, \"juta\":1_000_000, \"miliar\":1_000_000_000, \"milyar\":1_000_000_000}\n",
    "\n",
    "# Life/death\n",
    "RE_LIFE  = re.compile(r\"seumur\\s+h?idup\", re.I)\n",
    "RE_DEATH = re.compile(r\"(pidana|hukuman)\\s+mati\", re.I)\n",
    "\n",
    "# Mitigasi/aggravasi\n",
    "MITIGASI = [r\"belum\\s+pernah\\s+dihukum\", r\"menyesal\", r\"bersikap\\s+sopan\", r\"mengakui\\s+perbuatan\"]\n",
    "AGGRAV   = [r\"residivis\", r\"perbuatan\\s+meresahkan\", r\"tidak\\s+menyesal\"]\n",
    "\n",
    "LIFE_MONTHS    = 600.0\n",
    "MAX_REASONABLE = 720.0\n",
    "WINDOW_AFTER   = 300  # ambil sedikit ekor setelah kalimat pemicu\n",
    "\n",
    "def _to_num(s):\n",
    "    return float(str(s).replace(\".\",\"\").replace(\",\", \".\"))\n",
    "\n",
    "def _money_id(text: str) -> float:\n",
    "    \"\"\"Kembalikan denda dalam Rupiah (ambil angka terbesar yang muncul).\"\"\"\n",
    "    if not isinstance(text, str): \n",
    "        return 0.0\n",
    "    vals = []\n",
    "    for v in RE_RP_NUMBER.findall(text or \"\"):\n",
    "        try: vals.append(int(_to_num(v)))\n",
    "        except: pass\n",
    "    for v,unit in RE_SCALED_MONEY.findall(text or \"\"):\n",
    "        try: vals.append(int(_to_num(v) * SCALE[unit.lower()]))\n",
    "        except: pass\n",
    "    return float(max(vals)) if vals else 0.0\n",
    "\n",
    "def _months_from_span(span: str) -> float:\n",
    "    if not span: return np.nan\n",
    "    total = 0.0\n",
    "    y = RE_YEARS.search(span); m = RE_MONTHS.search(span); d = RE_DAYS.search(span)\n",
    "    if y: total += int(y.group(1)) * 12\n",
    "    if m: total += int(m.group(1))\n",
    "    if d: total += int(d.group(1)) / 30.0\n",
    "    return total if total > 0 else np.nan\n",
    "\n",
    "def _pick_best_duration(text: str, trigger: re.Pattern) -> float:\n",
    "    best = np.nan\n",
    "    for m in trigger.finditer(text or \"\"):\n",
    "        start, end = m.span()\n",
    "        chunk = (text or \"\")[start:end + WINDOW_AFTER]\n",
    "        v = _months_from_span(chunk)\n",
    "        if np.isnan(v) or v > MAX_REASONABLE: \n",
    "            continue\n",
    "        best = v if np.isnan(best) else max(best, v)\n",
    "    return best\n",
    "\n",
    "def _legal_refs_all(text: str):\n",
    "    return RE_PASAL_UU.findall(text or \"\")\n",
    "\n",
    "def _legal_ref_primary(text: str) -> str:\n",
    "    m = _legal_refs_all(text)\n",
    "    if not m: return \"UNKNOWN\"\n",
    "    with_year = [x for x in m if x[2]]\n",
    "    if with_year:\n",
    "        pasal,no,yr = with_year[0]\n",
    "        return f\"UU{no}/{yr}_pasal{pasal}\"\n",
    "    # fallback tanpa tahun -> pilih yang paling sering\n",
    "    from collections import Counter\n",
    "    cnt = Counter((p,n) for (p,n,_) in m)\n",
    "    (pasal,no), _ = cnt.most_common(1)[0]\n",
    "    return f\"UU{no}/XXXX_pasal{pasal}\"\n",
    "\n",
    "def _subsider_months(text: str) -> float:\n",
    "    m = re.search(r\"subsidi(?:air|er)\\s+(\\d+)\\s*(bulan|bln|hari)\", text or \"\", flags=re.I)\n",
    "    if not m: return 0.0\n",
    "    val, unit = m.groups()\n",
    "    val = int(val)\n",
    "    if unit.lower().startswith(\"hari\"): \n",
    "        return val/30.0\n",
    "    return float(val)\n",
    "\n",
    "def _count_keywords(text: str, patterns):\n",
    "    t = text or \"\"\n",
    "    return int(any(re.search(p, t, flags=re.I) for p in patterns))\n",
    "\n",
    "def extract_features_doc(raw_text: str, normalized_text: str = None):\n",
    "    \"\"\"\n",
    "    Return dict fitur TANPA bb_gram:\n",
    "      - amar_bulan, tuntutan_bulan\n",
    "      - denda_rp, subsider_bulan\n",
    "      - legal_ref_primary, legal_ref_count\n",
    "      - has_mitigasi, has_aggrav\n",
    "    \"\"\"\n",
    "    if not isinstance(raw_text, str) or not raw_text:\n",
    "        return {\n",
    "            \"amar_bulan\": np.nan, \"tuntutan_bulan\": np.nan,\n",
    "            \"denda_rp\": 0.0, \"subsider_bulan\": 0.0,\n",
    "            \"legal_ref_primary\": \"UNKNOWN\", \"legal_ref_count\": 0,\n",
    "            \"has_mitigasi\": 0, \"has_aggrav\": 0\n",
    "        }\n",
    "\n",
    "    t = normalized_text if isinstance(normalized_text, str) else raw_text\n",
    "\n",
    "    # life/death override\n",
    "    if RE_LIFE.search(raw_text) or RE_DEATH.search(raw_text):\n",
    "        amar = LIFE_MONTHS\n",
    "    else:\n",
    "        amar = _pick_best_duration(t, RE_AMAR_TRIG)\n",
    "\n",
    "    tuntut = _pick_best_duration(t, RE_TUNTUT_TRIG)\n",
    "    denda  = _money_id(raw_text)\n",
    "    subs   = _subsider_months(raw_text)\n",
    "\n",
    "    refs = _legal_refs_all(raw_text)\n",
    "    legal_primary = _legal_ref_primary(raw_text)\n",
    "    legal_count   = len(refs)\n",
    "\n",
    "    has_mitig = _count_keywords(raw_text, MITIGASI)\n",
    "    has_aggr  = _count_keywords(raw_text, AGGRAV)\n",
    "\n",
    "    return {\n",
    "        \"amar_bulan\": amar,\n",
    "        \"tuntutan_bulan\": tuntut,\n",
    "        \"denda_rp\": denda,\n",
    "        \"subsider_bulan\": subs,\n",
    "        \"legal_ref_primary\": legal_primary,\n",
    "        \"legal_ref_count\": legal_count,\n",
    "        \"has_mitigasi\": has_mitig,\n",
    "        \"has_aggrav\": has_aggr,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-23T11:59:17.920987Z",
     "iopub.status.idle": "2025-10-23T11:59:17.921267Z",
     "shell.execute_reply": "2025-10-23T11:59:17.921159Z",
     "shell.execute_reply.started": "2025-10-23T11:59:17.921144Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Apply to dataframe\n",
    "SRC = \"text_bert\" # bert cleaning result\n",
    "if \"_norm_text\" not in train.columns:\n",
    "    train[\"_norm_text\"] = train[SRC]\n",
    "    test[\"_norm_text\"] = test[SRC]\n",
    "\n",
    "feat_train = pd.DataFrame([extract_features_doc(rt, nt) for rt,nt in zip(train[SRC], train[\"_norm_text\"])])\n",
    "feat_test = pd.DataFrame([extract_features_doc(rt, nt) for rt,nt in zip(test[SRC], test[\"_norm_text\"])])\n",
    "\n",
    "train = pd.concat([train, feat_train], axis = 1)\n",
    "test = pd.concat([test, feat_test], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-23T11:59:17.921991Z",
     "iopub.status.idle": "2025-10-23T11:59:17.922261Z",
     "shell.execute_reply": "2025-10-23T11:59:17.922123Z",
     "shell.execute_reply.started": "2025-10-23T11:59:17.922108Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# check\n",
    "train.shape\n",
    "train.columns\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-23T11:59:17.924519Z",
     "iopub.status.idle": "2025-10-23T11:59:17.924759Z",
     "shell.execute_reply": "2025-10-23T11:59:17.924667Z",
     "shell.execute_reply.started": "2025-10-23T11:59:17.924657Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "\n",
    "# Encoding\n",
    "train[\"legal_ref_primary\"] = train[\"legal_ref_primary\"].astype(str)\n",
    "test[\"legal_ref_primary\"] = test[\"legal_ref_primary\"].astype(str)\n",
    "train[\"legal_ref_le\"], ref_vals = pd.factorize(train[\"legal_ref_primary\"])\n",
    "test[\"legal_ref_le\"] = test[\"legal_ref_primary\"].astype(pd.CategoricalDtype(categories=ref_vals)).cat.codes\n",
    "test[\"legal_ref_le\"] = test[\"legal_ref_le\"].replace(-1, ref_vals.size)\n",
    "\n",
    "# Data type normalization \n",
    "for col in [\"amar_bulan\", \"tuntutan_bulan\", \"subsider_bulan\"]:\n",
    "    train[col] = train[col].astype(\"float32\")\n",
    "    test[col] = test[col].astype(\"float32\")\n",
    "train[\"denda_rp\"] = train[\"denda_rp\"].astype(\"float64\")\n",
    "test[\"denda_rp\"] = test[\"denda_rp\"].astype(\"float64\")\n",
    "for col in [\"has_mitigasi\", \"has_aggrav\", \"legal_ref_count\", \"legal_ref_le\"]:\n",
    "    train[col] = train[col].astype(\"int32\")\n",
    "    test[col] = test[col].astype(\"int32\")\n",
    "\n",
    "feat_cols = [\n",
    "    \"amar_bulan\", \"tuntutan_bulan\", \"denda_rp\", \"subsider_bulan\", \"legal_ref_primary\",\n",
    "    \"legal_ref_count\", \"legal_ref_le\", \"has_mitigasi\", \"has_aggrav\"\n",
    "]\n",
    "\n",
    "# Merge id for join purpose\n",
    "id_col = \"doc_id\" if \"doc_id\" in train.columns else train.index.name or \"idx\"\n",
    "if id_col not in train.columns:\n",
    "    train = train.reset_index().rename(columns={\"index\":\"idx\"})\n",
    "    test = test.reset_index().rename(columns={\"index\":\"idx\"})\n",
    "    id_col = \"idx\"\n",
    "\n",
    "train_feats = train[[id_col] + feat_cols].copy()\n",
    "test_feats = test[[id_col] + feat_cols].copy()\n",
    "\n",
    "# save\n",
    "train_feats.to_parquet(\"train_feats.parquet\", index=False)\n",
    "test_feats.to_parquet(\"test_feats.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Feature Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-23T11:59:17.926299Z",
     "iopub.status.idle": "2025-10-23T11:59:17.926605Z",
     "shell.execute_reply": "2025-10-23T11:59:17.926468Z",
     "shell.execute_reply.started": "2025-10-23T11:59:17.926454Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1,2),\n",
    "    max_features=200_000,\n",
    "    min_df=2,\n",
    "    max_df=0.9,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "print(\"Fitting TF-IDF...\")\n",
    "Xtr_tfidf = tfidf.fit_transform(train[\"text_tfidf\"])\n",
    "Xte_tfidf = tfidf.transform(test[\"text_tfidf\"])\n",
    "Xtr_tfidf = csr_matrix(Xtr_tfidf, dtype=np.float32)\n",
    "Xte_tfidf = csr_matrix(Xte_tfidf, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-23T11:59:17.927747Z",
     "iopub.status.idle": "2025-10-23T11:59:17.928062Z",
     "shell.execute_reply": "2025-10-23T11:59:17.927927Z",
     "shell.execute_reply.started": "2025-10-23T11:59:17.927914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Domain Specific Numeric Features (tabular)\n",
    "\n",
    "if \"denda_rp\" in train.columns:\n",
    "    train[\"log1p_denda\"] = np.log1p(train[\"denda_rp\"].fillna(0))\n",
    "    test[\"log1p_denda\"] = np.log1p(test[\"denda_rp\"].fillna(0))\n",
    "else:\n",
    "    train[\"log1p_denda\"] = 0.0\n",
    "    test[\"log1p_denda\"] = 0.0\n",
    "\n",
    "num_cols = [\n",
    "    \"amar_bulan\", \"tuntutan_bulan\", \"subsider_bulan\", \"legal_ref_le\",\n",
    "    \"legal_ref_count\", \"has_mitigasi\", \"has_aggrav\", \"log1p_denda\",\n",
    "]\n",
    "\n",
    "train[\"text_len\"] = train[\"text_bert\"].str.len().fillna(0).astype(\"float32\")\n",
    "test[\"text_len\"] = test[\"text_bert\"].str.len().fillna(0).astype(\"float32\")\n",
    "num_cols.append(\"text_len\")\n",
    "\n",
    "for c in num_cols:\n",
    "    if c not in train.columns: train[c] = 0\n",
    "    if c not in test.columns: test[c] = 0\n",
    "    if train[c].dtype.kind not in \"fi\":\n",
    "        train[c] = pd.to_numeric(train[c], errors=\"coerce\")\n",
    "        test[c] = pd.to_numeric(test[c], errors=\"coerce\")\n",
    "\n",
    "train[num_cols] = train[num_cols].fillna(-1)\n",
    "test[num_cols] = test[num_cols].fillna(-1)\n",
    "\n",
    "num_tr = csr_matrix(train[num_cols].to_numpy(np.float32))\n",
    "num_te = csr_matrix(test[num_cols].to_numpy(np.float32))\n",
    "\n",
    "Xtr = hstack([Xtr_tfidf, num_tr], dtype=np.float32).tocsr()\n",
    "Xte = hstack([Xte_tfidf, num_te], dtype=np.float32).tocsr()\n",
    "print(\"Shapes:\", Xtr.shape, Xte.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-23T11:59:17.929929Z",
     "iopub.status.idle": "2025-10-23T11:59:17.930324Z",
     "shell.execute_reply": "2025-10-23T11:59:17.930156Z",
     "shell.execute_reply.started": "2025-10-23T11:59:17.930141Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Sematic -> embeddings\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# load embeddings\n",
    "data = np.load(\"/kaggle/input/bert-embeddings/bert_embeddings.npz\")\n",
    "E_tr, E_te = data[\"E_tr\"], data[\"E_te\"]\n",
    "print(\"Embedding shapes:\", E_tr.shape, E_te.shape)\n",
    "\n",
    "# sanity check\n",
    "assert E_tr.shape[0] == len(train), \"Mismatch: n_train vs E_tr\"\n",
    "assert E_te.shape[0] == len(test),  \"Mismatch: n_test vs E_te\"\n",
    "\n",
    "# cast to float32 to minimize RAM\n",
    "E_tr = E_tr.astype(np.float32, copy=False)\n",
    "E_te = E_te.astype(np.float32, copy=False)\n",
    "\n",
    "# SVD to compress dimension\n",
    "USE_SVD = True\n",
    "svd_dim = 256\n",
    "if USE_SVD:\n",
    "    print(f\"SVD reducing BERT from {E_tr.shape[1]} -> {svd_dim} ...\")\n",
    "    svd = TruncatedSVD(n_components=svd_dim, random_state=42)\n",
    "    E_tr = svd.fit_transform(E_tr).astype(np.float32)\n",
    "    E_te = svd.transform(E_te).astype(np.float32)\n",
    "\n",
    "Etr_sp = csr_matrix(E_tr)\n",
    "Ete_sp = csr_matrix(E_te)\n",
    "\n",
    "Xtr = hstack([Xtr, Etr_sp], dtype=np.float32).tocsr()\n",
    "Xte = hstack([Xte, Ete_sp], dtype=np.float32).tocsr()\n",
    "\n",
    "print(\"Shapes with BERT:\", Xtr.shape, Xte.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target-Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-23T11:59:17.931376Z",
     "iopub.status.idle": "2025-10-23T11:59:17.931801Z",
     "shell.execute_reply": "2025-10-23T11:59:17.931638Z",
     "shell.execute_reply.started": "2025-10-23T11:59:17.931623Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "# determine the \"legal_ref_primary\"\n",
    "UU_COL = \"legal_ref_primary\" if \"legal_ref_primary\" in train.columns else \"legal_ref\"\n",
    "uu_train = train[UU_COL].astype(str).fillna(\"UNKNOWN\")\n",
    "uu_test  = test [UU_COL].astype(str).fillna(\"UNKNOWN\")\n",
    "\n",
    "# target and group kfold setup\n",
    "CAND_TGT = [\"lama hukuman (bulan)\", \"lama_hukuman_bulan\"]\n",
    "TARGET_COL = next(c for c in CAND_TGT if c in train.columns)\n",
    "\n",
    "CLIP_TRAIN_MAX = 240\n",
    "y_raw  = train[TARGET_COL].astype(float).values\n",
    "y_clip = np.clip(y_raw, None, CLIP_TRAIN_MAX).astype(np.float32)\n",
    "y_log  = np.log1p(y_clip)\n",
    "\n",
    "groups = train.get(\"legal_ref_primary\", train.get(\"legal_ref\", \"UNK\")).astype(str).values\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "splits = list(gkf.split(Xtr, y_log, groups))\n",
    "\n",
    "# global mean and target encoding initialization\n",
    "global_mean = float(y_clip.mean())\n",
    "te_oof = np.zeros(len(train), dtype=np.float32)\n",
    "te_test_blend = np.zeros(len(test), dtype=np.float32)\n",
    "\n",
    "for tr_idx, va_idx in splits:\n",
    "    g_tr = uu_train.iloc[tr_idx]\n",
    "    y_tr = y_clip[tr_idx]\n",
    "\n",
    "    # mean per group from fold-train\n",
    "    grp_mean = (\n",
    "        pd.DataFrame({\"g\": g_tr.values, \"y\": y_tr})\n",
    "        .groupby(\"g\")[\"y\"].mean()\n",
    "    )\n",
    "\n",
    "    # assign to VAL based on mapping from fold-train gorup\n",
    "    te_oof[va_idx] = (\n",
    "        uu_train.iloc[va_idx].map(grp_mean).fillna(global_mean)\n",
    "        .to_numpy(dtype=np.float32)\n",
    "    )\n",
    "\n",
    "    # for test\n",
    "    te_fold_test = (\n",
    "        uu_test.map(grp_mean).fillna(global_mean)\n",
    "        .to_numpy(dtype=np.float32)\n",
    "    )\n",
    "    te_test_blend += te_fold_test / len(splits)\n",
    "\n",
    "# shrinkage to global mean\n",
    "counts = uu_train.value_counts()\n",
    "cnt_tr = uu_train.map(counts).astype(float).to_numpy()\n",
    "cnt_te = uu_test.map(counts).fillna(0).astype(float).to_numpy()\n",
    "\n",
    "m = 50.0  # prior strength\n",
    "te_oof  = (cnt_tr/(cnt_tr+m))*te_oof  + (m/(cnt_tr+m))*global_mean\n",
    "te_test_blend = (cnt_te/(cnt_te+m))*te_test_blend + (m/(cnt_te+m))*global_mean\n",
    "\n",
    "# add TE to main feature\n",
    "Xtr = hstack([Xtr, csr_matrix(te_oof.reshape(-1,1))], format=\"csr\")\n",
    "Xte = hstack([Xte, csr_matrix(te_test_blend.reshape(-1,1))], format=\"csr\")\n",
    "\n",
    "print(\"TE feature added (aligned with GroupKFold). Shapes:\", Xtr.shape, Xte.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-23T11:59:17.932688Z",
     "iopub.status.idle": "2025-10-23T11:59:17.933030Z",
     "shell.execute_reply": "2025-10-23T11:59:17.932884Z",
     "shell.execute_reply.started": "2025-10-23T11:59:17.932867Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train LGBM + BERT features\n",
    "import numpy as np, lightgbm as lgb, pandas as pd\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Target & CV setup\n",
    "CAND_TGT = [\"lama hukuman (bulan)\", \"lama_hukuman_bulan\"]\n",
    "TARGET_COL = next(c for c in CAND_TGT if c in train.columns)\n",
    "\n",
    "CLIP_TRAIN_MAX = 240\n",
    "y_raw  = train[TARGET_COL].astype(float).values\n",
    "y_clip = np.clip(y_raw, None, CLIP_TRAIN_MAX).astype(np.float32)\n",
    "y_log  = np.log1p(y_clip)\n",
    "\n",
    "groups = train.get(\"legal_ref_primary\", train.get(\"legal_ref\",\"UNK\")).astype(str).values\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "splits = list(gkf.split(Xtr, y_log, groups))\n",
    "\n",
    "# bobot ringan utk tail\n",
    "q90, q95 = np.quantile(y_clip, [0.90, 0.95])\n",
    "w = np.ones_like(y_clip, dtype=np.float32)\n",
    "w[y_clip>q90] *= 1.25; w[y_clip>q95] *= 1.5\n",
    "\n",
    "# LightGBM params\n",
    "lgb_params = dict(\n",
    "    objective=\"huber\",   \n",
    "    alpha=0.7,\n",
    "    metric=\"rmse\",\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=64,\n",
    "    max_bin=63,\n",
    "    min_data_in_leaf=60,\n",
    "    feature_fraction=0.8,\n",
    "    bagging_fraction=0.7,\n",
    "    bagging_freq=1,\n",
    "    lambda_l2=1.0,\n",
    "    force_col_wise=True,\n",
    "    num_threads=4,\n",
    "    verbose=-1,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# train\n",
    "oof = np.zeros(len(train), dtype=np.float32)\n",
    "pred_te = np.zeros(len(test), dtype=np.float32)\n",
    "\n",
    "print(\"Training LGBM (dengan fitur BERT)...\")\n",
    "for fold, (tr_idx, va_idx) in enumerate(splits, 1):\n",
    "    dtr = lgb.Dataset(Xtr[tr_idx], label=y_log[tr_idx], weight=w[tr_idx], free_raw_data=False)\n",
    "    dva = lgb.Dataset(Xtr[va_idx], label=y_log[va_idx], free_raw_data=False)\n",
    "\n",
    "    model = lgb.train(\n",
    "        lgb_params, dtr, num_boost_round=6000,\n",
    "        valid_sets=[dva], valid_names=[\"val\"],\n",
    "        callbacks=[lgb.early_stopping(400, verbose=False), lgb.log_evaluation(250)]\n",
    "    )\n",
    "\n",
    "    va = np.clip(np.expm1(model.predict(Xtr[va_idx], num_iteration=model.best_iteration)), 0, CLIP_TRAIN_MAX)\n",
    "    te = np.clip(np.expm1(model.predict(Xte,        num_iteration=model.best_iteration)), 0, CLIP_TRAIN_MAX)\n",
    "\n",
    "    oof[va_idx] = va.astype(np.float32)\n",
    "    pred_te += (te.astype(np.float32) / gkf.n_splits)\n",
    "\n",
    "    rmse = mean_squared_error(y_clip[va_idx], va, squared=False)\n",
    "    print(f\"Fold {fold} RMSE: {rmse:.4f}\")\n",
    "\n",
    "rmse_oof = mean_squared_error(y_clip, oof, squared=False)\n",
    "print(f\"OOF RMSE (LGBM + BERT): {rmse_oof:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-23T11:59:17.933802Z",
     "iopub.status.idle": "2025-10-23T11:59:17.934107Z",
     "shell.execute_reply": "2025-10-23T11:59:17.933968Z",
     "shell.execute_reply.started": "2025-10-23T11:59:17.933955Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# fit global calibration in OOF\n",
    "cal = LinearRegression().fit(oof.reshape(-1,1), y_clip)\n",
    "a, b = float(cal.coef_[0]), float(cal.intercept_)\n",
    "print(f\"[Calib] y ≈ {a:.4f} * y_pred + {b:.4f}\")\n",
    "\n",
    "SUB_CLIP_MAX = 360\n",
    "pred_cal = np.clip(a*pred_te + b, 0, SUB_CLIP_MAX).astype(np.float32)\n",
    "\n",
    "if 'sample_submission' in globals():\n",
    "    sub = sample_submission.copy()\n",
    "    sub_col = sub.columns[-1]\n",
    "    sub[sub_col] = pred_cal\n",
    "else:\n",
    "    id_col = 'doc_id' if 'doc_id' in test.columns else ('id' if 'id' in test.columns else test.columns[0])\n",
    "    sub_col = \"lama hukuman (bulan)\"\n",
    "    sub = pd.DataFrame({id_col: test[id_col].values, sub_col: pred_cal})\n",
    "\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Saved -> submission.csv | shape:\", sub.shape)\n",
    "print(sub.head())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8168961,
     "sourceId": 12910428,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8168973,
     "sourceId": 12910448,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8168982,
     "sourceId": 12910460,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8189735,
     "sourceId": 12941631,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8190350,
     "sourceId": 12942502,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8552908,
     "sourceId": 13473152,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
